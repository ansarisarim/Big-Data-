
sudo apt update && sudo apt upgrade -y

# Take Control 

sudo -i
passwd
exit

# Create a Hadoop user for accessing HDFS	

sudo addgroup hdfs
sudo adduser --ingroup hdfs hdfs
sudo adduser hdfs sudo
sudo su hdfs

# Install Java 8

sudo apt install openjdk-8-jdk -y

# Install Hadoop

wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.5/hadoop-2.7.5.tar.gz -P ~/Downloads
sudo tar zxvf ~/Downloads/hadoop-* -C /usr/local
sudo mv /usr/local/hadoop-* /usr/local/hadoop

# Set Environment Variable

readlink -f $(which java)

cat >>$HOME/.bashrc <<EOL                                        
# -- HADOOP ENVIRONMENT VARIABLES START -- #
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/usr/local/hadoop
export PATH=\$PATH:\$HADOOP_HOME/bin
export PATH=\$PATH:\$HADOOP_HOME/sbin
export PATH=\$PATH:/usr/local/hadoop/bin/
export HADOOP_MAPRED_HOME=\$HADOOP_HOME
export HADOOP_COMMON_HOME=\$HADOOP_HOME
export HADOOP_HDFS_HOME=\$HADOOP_HOME
export YARN_HOME=\$HADOOP_HOME
export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop
export PDSH_RCMD_TYPE=ssh
# -- HADOOP ENVIRONMENT VARIABLES END -- #
EOL

 exec bash

sudo chown -R ubuntu:ubuntu /usr/local/hadoop
                                   
#Update hadoop-env.sh

sudo su -c 'echo export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo su -c 'echo export HADOOP_LOG_DIR=/var/log/hadoop/ >> /usr/local/hadoop/etc/hadoop/hadoop-env.sh'

sudo mkdir /var/log/hadoop/

sudo chown -R ubuntu:ubuntu /var/log/hadoop/

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Disable SELINUX 

sudo apt-get install selinux-utils

setenforce 0                                     {is command hu uska mode change karte hai}

cat /etc/selinux/config

SELINUX=disabled
SELINUXTYPE=targeted
SETLOCALDEFS=0

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Disable IPV6 

cat /proc/sys/net/ipv6/conf/all/disable_ipv6

sudo sysctl -p                                    {Modifying Kernel at runtime}

sudo su -c 'cat >>/etc/sysctl.conf <<EOL
net.ipv6.conf.all.disable_ipv6 =1
net.ipv6.conf.default.disable_ipv6 =1
net.ipv6.conf.lo.disable_ipv6 =1
EOL'
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Disable FireWall iptables

sudo iptables -L -n -v 

sudo iptables-save > firewall.rules 

The Uncomplicated Firewall or ufw is the configuration tool for iptables that comes by default on Ubuntu

sudo ufw status verbose 

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Disabling Transparent Hugepage Compaction

#Red Hat/CentOS: /sys/kernel/mm/redhat_transparent_hugepage/defrag

#Ubuntu/Debian, OEL, SLES: /sys/kernel/mm/transparent_hugepage/defrag

cat /sys/kernel/mm/transparent_hugepage/defrag

sudo sed -i '/exit 0/d' /etc/rc.local

sudo su -c 'cat >>/etc/rc.local <<EOL
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
  echo never > /sys/kernel/mm/transparent_hugepage/enabled
fi
if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
   echo never > /sys/kernel/mm/transparent_hugepage/defrag 
fi
exit 0
EOL'

sudo -i

source /etc/rc.local

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Set Swappiness

sudo sysctl -a | grep vm.swappiness

sudo su -c 'cat >>/etc/sysctl.conf <<EOL
'vm.swappiness=0'
EOL'

sudo sysctl -p

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Configure NTP 

timedatectl status
timedatectl list-timezones
timedatectl list-timezones | grep Asia/Kolkata
sudo timedatectl set-timezone Asia/Kolkata
timedatectl status
sudo ntpq -p
sudo apt-get install ntp -y
timedatectl status

sudo nano /etc/ntp.conf
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Root Reserved Space

mkfs.ext4  /dev/xvda1 ( filesystem is not suppose to be mounted)

sudo tune2fs -m 0 /dev/xvda1

sudo file -sL /dev/xvda1

lsblk
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Update core-site.xml        {Indexing ka kaam karta hai}

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/core-site.xml                                                                                                                     
sudo su -c 'cat >> /usr/local/hadoop/etc/hadoop/core-site.xml <<EOL
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://nn:9000</value>
  </property>
</configuration>

EOL'
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Update hdfs-site.xml on name node    {Replication par kaam karenga} 

mkdir -p /usr/local/hadoop/data/hdfs/namenode          {We are making directory for namrnode}

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/hdfs-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/hdfs-site.xml <<EOL
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
<property> 
   <name>dfs.secondary.http.address</name>
   <value>nn:50090</value>
   <description>SecondaryNameNodeHostname</description>
</property>
 <property>
    <name>dfs.datanode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>
EOL'

 mkdir /usr/local/hadoop/data/hdfs/namenode
 mkdir /usr/local/hadoop/data/hdfs/datanode

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Update yarn-site.xml

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/yarn-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/yarn-site.xml <<EOL

<configuration>
<!-- Site specific YARN configuration properties -->
 <property>
     <name>yarn.nodemanager.aux-services</name>
     <value>mapreduce_shuffle</value>
   </property> 
   <property>
     <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
     <value>org.apache.hadoop.mapred.ShuffleHandler</value>
   </property>
   <property>
     <name>yarn.resourcemanager.hostname</name>
     <value>rm</value>
   </property>
</configuration>
EOL'

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Update mapred-site.xml               {Mapred par logiacal daemons kaha hai bataayenga}

cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml             {Hum Mpared template se unnessecary things kat kar                                                                                                                      paste kar dete hai} 

sudo sed -i '/<configuration>/,/<\/configuration>/d' /usr/local/hadoop/etc/hadoop/mapred-site.xml

sudo su -c 'cat >>/usr/local/hadoop/etc/hadoop/mapred-site.xml <<EOL
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>rm:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
EOL'

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#create master

echo snn >>$HADOOP_CONF_DIR/masters                             {$HADOOP_CONF_DIR/- yeah wala varaible pat store krta hai}

#Update slaves

cat >>$HADOOP_CONF_DIR/slaves<<EOL
1dn
2dn
3dn
EOL

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Configure SSH Password less logins 

sudo su -c touch /home/ubuntu/.ssh/config; echo -e \ "Host *\n StrictHostKeyChecking no\n  UserKnownHostsFile=/dev/null" \ > ~/.ssh/config

echo -e  'y\n'| ssh-keygen -t rsa -P "" -f $HOME/.ssh/id_rsa

cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Get a key

scp -i alpha.pem alpha.pem ubuntu@publicip:/home/ubuntu/.ssh

echo 'eval 'ssh-agent' ssh-add /home/ubuntu/.ssh/alfa-pemkey.pem' >>~/.profile

source .profile

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------

#Create a AMI and lauch instane from ami 

sudo apt-get update && sudo apt-get dist-upgrade -y

sudo apt install pdsh -y

#Configure pdsh 

sudo su -c 'cat>>/etc/genders<<EOL
nn
snn
rm
1dn
2dn
3dn
EOL'

#We already configure bash environment for pdsh

export PDSH_RCMD_TYPE=ssh                                          {PDSH se hum multiple remote hosts of parallely command dete hai}

#Update /etc/hosts

make image & launch 5 instances



sudo su -c 'cat>>/etc/hosts<<EOL

172.31.10.231 nn
172.31.1.226 snn
172.31.5.175 rm
172.31.5.210 1dn
172.31.7.105 2dn
172.31.11.106 3dn
EOL'

#Temporarily Changing the permission of etc/hosts file

pdsh -x nn -a exec sudo chown ubuntu:ubuntu /etc/hosts               {nn ko chodkar hum sab ke owner ship change kar rahe hai}

#Fire this command for nn to update al machine execept nn

pdsh -x nn -a exec 'cat>>/etc/hosts<<EOL
172.31.10.231 nn
172.31.1.226 snn
172.31.5.175 rm
172.31.5.210 1dn
172.31.7.105 2dn
172.31.11.106 3dn
EOL'

#Give ownership back to root 

pdsh -x nn -a exec sudo chown root:root /etc/hosts

#Format Namenode 

hdfs namenode -format

#Start the cluster

ssh nn 
start-dfs.sh

ssh rm
start-yarn.sh

$HADOOP_HOME/sbin/mr-jobhistory-daemons.sh start historyserver

ssh nn
pdsh -a jps 
pdsh -a jps | sort

#BENCHMARKING

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar teragen 5000 random-data

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar terasort random-data sorted-data

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 5MB

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-*-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 5MB

NameNode 
http://nn_host:port/ 
Default HTTP port is 50070. 
ResourceManager 
http://rm_host:port/ 
Default HTTP port is 8088. 
MapReduce JobHistory Server 
http://jhs_host:port/ 
Default HTTP port is 19888. 


SecondaryNameNode 
http://snn_host:port/ 
Default HTTP port is 50090
DataNode
http://dn_host:port/ 
Default HTTP port is 50075. 

http://jhs_host:port/ 
Default HTTP port is 19888. 

hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar pi 10 100000














